# âœ… Delta Table Automation Project â€” End-to-End

## ğŸ”§ Tools
- Databricks Community Edition
- Local Python Script (for sending email)

## ğŸ“¦ Features

1. **Data Generation**
   - Generate `1000` fake rows using libraries such as `Faker` or `random`.
   - Ensure schema matches target Delta table.

2. **Delta Table Append**
   - Use **Delta Lake API** to append generated data to existing Delta table.
   - Table versioning enabled for tracking changes.

3. **Data Export**
   - Query the **latest 100 rows** from the Delta table.
   - Export result as a **CSV file**.

4. **CSV Download**
   - Manually download the exported CSV from **DBFS (Databricks File System)**.
   - Use Databricks CLI or notebook commands.

5. **Email Automation**
   - Send email from **local system**.
   - Attach the downloaded CSV file.
   - Use `smtplib`, `email.mime`, or any preferred mail automation package.

# ğŸ”· STEP 1: Generate & Append 1000 Fake Rows to Delta Table

---

### ğŸ§¾ Python Code

```python
from pyspark.sql.functions import current_timestamp
from faker import Faker
import pandas as pd
from delta.tables import DeltaTable

# Initialize Faker
fake = Faker()
Faker.seed(42)

# 1. Generate 1000 fake rows
def generate_fake_data(n):
    return pd.DataFrame([{
        "name": fake.name(),
        "address": fake.address().replace('\n', ', '),
        "email": fake.email(),
        "ingestion_time": pd.Timestamp.now()
    } for _ in range(n)])

pdf = generate_fake_data(1000)
df = spark.createDataFrame(pdf)

# 2. Define Delta table path
delta_path = "dbfs:/tmp/user_delta_table"

# 3. Append or Create Delta table
if DeltaTable.isDeltaTable(spark, delta_path):
    delta_table = DeltaTable.forPath(spark, delta_path)
    delta_table.alias("old").merge(
        df.alias("new"), "old.email = new.email"
    ).whenNotMatchedInsertAll().execute()
else:
    df.write.format("delta").mode("overwrite").save(delta_path)

print("âœ… Delta table updated with 1000 rows.")
--- 
# ğŸ”· STEP 2: Read Full Delta Table & Show Latest 100 Rows


### ğŸ§¾ Python Code

```python
# Load full Delta table
df_all = spark.read.format("delta").load(delta_path)

# Show latest 100 rows
from pyspark.sql.functions import col
df_all.orderBy(col("ingestion_time").desc()).show(100, truncate=False)

print(f"âœ… Total rows in Delta Table: {df_all.count()}")

## ğŸ”· STEP 1: Generate & Append 1000 Fake Rows to Delta Table

ğŸ“ **Run this in a Databricks CE Notebook**

```python
from pyspark.sql.functions import current_timestamp
from faker import Faker
import pandas as pd
from delta.tables import DeltaTable

# Initialize Faker
fake = Faker()
Faker.seed(42)

# 1. Generate 1000 fake rows
def generate_fake_data(n):
    return pd.DataFrame([{
        "name": fake.name(),
        "address": fake.address().replace('\n', ', '),
        "email": fake.email(),
        "ingestion_time": pd.Timestamp.now()
    } for _ in range(n)])

pdf = generate_fake_data(1000)
df = spark.createDataFrame(pdf)

# 2. Define Delta table path
delta_path = "dbfs:/tmp/user_delta_table"

# 3. Append or Create Delta table
if DeltaTable.isDeltaTable(spark, delta_path):
    delta_table = DeltaTable.forPath(spark, delta_path)
    delta_table.alias("old").merge(
        df.alias("new"), "old.email = new.email"
    ).whenNotMatchedInsertAll().execute()
else:
    df.write.format("delta").mode("overwrite").save(delta_path)

print("âœ… Delta table updated with 1000 rows.")
ğŸ”· STEP 2: Read Full Delta Table & Show Latest 100 Rows
python
Copy
Edit
# Load full Delta table
df_all = spark.read.format("delta").load(delta_path)

# Show latest 100 rows
from pyspark.sql.functions import col
df_all.orderBy(col("ingestion_time").desc()).show(100, truncate=False)

print(f"âœ… Total rows in Delta Table: {df_all.count()}")
ğŸ”· STEP 3: Export Top 100 Rows as CSV (Single File)
python
Copy
Edit
# Save latest 100 rows to single CSV part file
df_all.orderBy("ingestion_time", ascending=False).limit(100) \
    .coalesce(1).write.mode("overwrite").option("header", "true") \
    .csv("dbfs:/tmp/user_data_latest")

print("âœ… Exported top 100 rows to CSV.")
ğŸ”· STEP 4: âœ… Manually Download the CSV File
Go to Databricks Sidebar â†’ Data â†’ DBFS

Navigate to:

bash
Copy
Edit
/tmp/user_data_latest/
Locate the file that starts with:

Copy
Edit
part-00000-
Right-click â†’ Download

Rename the file to:

Copy
Edit
user_data_latest.csv
âœ… You now have the CSV file on your local machine.
